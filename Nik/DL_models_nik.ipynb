{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d313ea48-91ea-453e-aadd-e0aabe3527a9",
   "metadata": {},
   "source": [
    "## CUDA and SPLIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5b18c44-e12a-4cb6-94ef-b2e3eba6334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.empty_cache()\n",
    "SPLIT_BY = 'actor'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eaad13-b4a4-4d1b-91f5-27c13d1c4e80",
   "metadata": {},
   "source": [
    "## normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27faa1a1-ab9a-4c27-95eb-4fd4ce8a2ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of global mean: torch.Size([1, 1])\n",
      "Shape of global standard dev: torch.Size([1, 1])\n",
      "Shape of channel mean: torch.Size([131, 1])\n",
      "Shape of channel standard dev: torch.Size([131, 1])\n"
     ]
    }
   ],
   "source": [
    "from statMLlib import DatasetWrapper \n",
    "import imp\n",
    "imp.reload(DatasetWrapper)\n",
    "from statMLlib.DatasetWrapper import RAVDESSFeatureDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class StatsRecorder:\n",
    "    def __init__(self, red_dims=(0,1,2)):\n",
    "        \"\"\"Accumulates normalization statistics across mini-batches.\n",
    "        ref: http://notmatthancock.github.io/2017/03/23/simple-batch-stat-updates.html\n",
    "        \"\"\"\n",
    "        self.red_dims = red_dims # which mini-batch dimensions to average over\n",
    "        self.nobservations = 0   # running number of observations\n",
    "\n",
    "    def update(self, data):\n",
    "        \"\"\"\n",
    "        data: ndarray, shape (nobservations, ndimensions)\n",
    "        \"\"\"\n",
    "        # initialize stats and dimensions on first batch\n",
    "        if self.nobservations == 0:\n",
    "            self.mean = data.mean(dim=self.red_dims, keepdim=True)\n",
    "            self.std  = data.std (dim=self.red_dims,keepdim=True)\n",
    "            self.nobservations = data.shape[0]\n",
    "            self.ndimensions   = data.shape[1]\n",
    "        else:\n",
    "            if data.shape[1] != self.ndimensions:\n",
    "                raise ValueError('Data dims do not match previous observations.')\n",
    "            \n",
    "            # find mean of new mini batch\n",
    "            newmean = data.mean(dim=self.red_dims, keepdim=True)\n",
    "            newstd  = data.std(dim=self.red_dims, keepdim=True)\n",
    "            \n",
    "            # update number of observations\n",
    "            m = self.nobservations * 1.0\n",
    "            n = data.shape[0]\n",
    "\n",
    "            # update running statistics\n",
    "            tmp = self.mean\n",
    "            self.mean = m/(m+n)*tmp + n/(m+n)*newmean\n",
    "            self.std  = m/(m+n)*self.std**2 + n/(m+n)*newstd**2 +\\\n",
    "                        m*n/(m+n)**2 * (tmp - newmean)**2\n",
    "            self.std  = torch.sqrt(self.std)\n",
    "                                 \n",
    "            # update total number of seen samples\n",
    "            self.nobservations += n\n",
    "\n",
    "\n",
    "root_features='/home/spongebob*/statML_project/RAVDESS/RAVDESS-emotions-speech-audio-only-master/Audio_Speech_Actors_01-24/FeaturesAll/'\n",
    "\n",
    "train_iter = RAVDESSFeatureDataset(split='train',split_by=SPLIT_BY, root_dir=root_features)#Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "train_dataloader = DataLoader(train_iter, batch_size=8, shuffle=True)\n",
    "\n",
    "# create recorders\n",
    "global_stats  = StatsRecorder()\n",
    "channel_stats = StatsRecorder(red_dims=(0,2))\n",
    "\n",
    "# step through the training dataset\n",
    "with torch.no_grad():\n",
    "    for idx,(x,y) in enumerate(iter(train_dataloader)):\n",
    "        # print(x.shape)\n",
    "        # print((torch.mean(x, axis=1)))\n",
    "        # update normalization statistics\n",
    "        # x=(x-torch.mean(x))/torch.std(x)\n",
    "        global_stats.update(x)\n",
    "        channel_stats.update(x)\n",
    "    \n",
    "# parse out both sets of stats\n",
    "global_mean,global_std = global_stats.mean,global_stats.std\n",
    "global_mean=global_mean.squeeze(0)\n",
    "global_std=global_std.squeeze(0)\n",
    "\n",
    "\n",
    "channel_mean,channel_std = channel_stats.mean,channel_stats.std\n",
    "channel_mean=channel_mean.squeeze(0)\n",
    "channel_std=channel_std.squeeze(0)\n",
    "\n",
    "print(f'Shape of global mean: {global_mean.shape}')\n",
    "print(f'Shape of global standard dev: {global_std.shape}')\n",
    "\n",
    "print(f'Shape of channel mean: {channel_mean.shape}')\n",
    "print(f'Shape of channel standard dev: {channel_std.shape}')\n",
    "# print(channel_mean)\n",
    "global_mean.to(DEVICE)\n",
    "global_std.to(DEVICE)\n",
    "\n",
    "channel_mean.to(DEVICE)\n",
    "channel_std.to(DEVICE)\n",
    "\n",
    "# print(channel_mean)\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a94d4a-a8c8-4305-b81c-c86fc212ada8",
   "metadata": {},
   "source": [
    "## check sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f19fe7b-14cd-460e-9e0f-0958de3a945b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 131, 81])\n",
      "tensor(-0.0275, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from statMLlib import DatasetWrapper \n",
    "import imp\n",
    "imp.reload(DatasetWrapper)\n",
    "from statMLlib.DatasetWrapper import RAVDESSFeatureDataset\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "root_features='/home/spongebob*/statML_project/RAVDESS/RAVDESS-emotions-speech-audio-only-master/Audio_Speech_Actors_01-24/FeaturesAll/'\n",
    "\n",
    "train_iter = RAVDESSFeatureDataset(split='train',split_by=SPLIT_BY, root_dir=root_features, mean=channel_mean, std= channel_std)#Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "for src, tgt in train_dataloader:\n",
    "    src = src.to(DEVICE)\n",
    "    tgt = tgt.to(DEVICE)\n",
    "    \n",
    "    print(src.shape)\n",
    "#     print(tgt.shape)\n",
    "#     print(torch.mean(src, axis=1).shape)\n",
    "    \n",
    "\n",
    "    print(torch.mean(src[0][0]))\n",
    "    fig=plt.figure(figsize=(8, 8))\n",
    "    # plt.plot(src.cpu().numpy()[1])\n",
    "    imgplot = plt.imshow(src.cpu().numpy()[1].T)\n",
    "    \n",
    "#     print(std)\n",
    "#     print(src_norm.cpu().numpy()[5].T)\n",
    "\n",
    "    break\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa73e7bd-a07f-40f2-85ec-0400cef3ce8c",
   "metadata": {},
   "source": [
    "## Train Eval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61117f98-88da-42ac-bb0f-99a9b4fe9deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from statMLlib.DatasetWrapper import RAVDESSFeatureDataset\n",
    "\n",
    "root_features='/home/spongebob*/statML_project/RAVDESS/RAVDESS-emotions-speech-audio-only-master/Audio_Speech_Actors_01-24/FeaturesAll/'\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "\n",
    "    train_iter = RAVDESSFeatureDataset(split='train',split_by=SPLIT_BY, root_dir=root_features, mean=MEAN, std= STD)#Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "        \n",
    "#         if(NORM=='channel'):\n",
    "#             mean=channel_mean.repeat(src.shape[0],1,src.shape[2]).to(DEVICE)\n",
    "#             std=channel_std.repeat(src.shape[0],1,src.shape[2]).to(DEVICE)\n",
    "        \n",
    "#         elif NORM == 'global':\n",
    "#             mean=global_mean.repeat(src.shape[0],src.shape[1],src.shape[2]).to(DEVICE)\n",
    "#             std=global_std.repeat(src.shape[0],src.shape[1],src.shape[2]).to(DEVICE)\n",
    "            \n",
    "#         else:\n",
    "#             mean=torch.tensor(0).to(DEVICE)\n",
    "#             std=torch.tensor(1).to(DEVICE)\n",
    "            \n",
    "#         # src=(src-torch.mean(src, axis=1))/torch.std(src, axis=1)\n",
    "#         src_norm=(src-mean)/std\n",
    "        \n",
    "        logits = model(src)\n",
    "        # print(logits.shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "       \n",
    "\n",
    "        loss = loss_fn(logits, tgt)\n",
    "        # print(loss)\n",
    "        # loss = loss_fn(logits, tgt_out)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    \n",
    "    val_iter = RAVDESSFeatureDataset(split='valid',split_by=SPLIT_BY, root_dir=root_features, mean=MEAN, std= STD)#Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "        \n",
    "#         if NORM=='channel':\n",
    "#             mean=channel_mean.repeat(src.shape[0],1,src.shape[2]).to(DEVICE)\n",
    "#             std=channel_std.repeat(src.shape[0],1,src.shape[2]).to(DEVICE)\n",
    "        \n",
    "#         elif NORM == 'global':\n",
    "#             mean=global_mean.repeat(src.shape[0],src.shape[1],src.shape[2]).to(DEVICE)\n",
    "#             std=global_std.repeat(src.shape[0],src.shape[1],src.shape[2]).to(DEVICE)\n",
    "            \n",
    "#         else:\n",
    "#             mean=torch.tensor(0).to(DEVICE)\n",
    "#             std=torch.tensor(1).to(DEVICE)\n",
    "            \n",
    "#         # src=(src-torch.mean(src))/torch.std(src)\n",
    "#         src_norm=(src-mean)/std\n",
    "\n",
    "        logits = model(src)\n",
    "  \n",
    "        loss = loss_fn(logits, tgt)\n",
    "     \n",
    "        losses += loss.item()\n",
    "    \n",
    "    \n",
    "    \n",
    "    return losses / len(val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920ada28-39e6-445b-9158-8ce68bfa78d2",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7704dce6-eb1b-4ea1-86b1-8ce20bc312fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Ravdess_MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, tgt_class_cnt, num_of_layers):\n",
    "        super(Ravdess_MLP, self).__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.input_size=input_size\n",
    "        self.tgt_class_cnt=tgt_class_cnt\n",
    "        \n",
    "        self.hidden_dim=int(self.input_size/2)\n",
    "                               \n",
    "        self.linear_relu_stack1 = nn.Sequential(\n",
    "\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.BatchNorm1d(input_size),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(input_size, self.hidden_dim),\n",
    "            nn.BatchNorm1d(self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "        \n",
    "        self.hidden = []\n",
    "        for k in range(num_of_layers):\n",
    "            self.hidden.append(nn.Linear(self.hidden_dim, self.hidden_dim))\n",
    "            self.hidden.append(nn.BatchNorm1d(self.hidden_dim))\n",
    "            self.hidden.append(nn.ReLU())\n",
    "            # self.hidden.append(nn.Dropout(0.1))\n",
    "\n",
    "       \n",
    "        self.linear_relu_stack2 = nn.Sequential(*self.hidden)\n",
    "        \n",
    "        self.linear_relu_stack3 = nn.Sequential(\n",
    "  \n",
    "            nn.Linear(self.hidden_dim, int(self.hidden_dim/2)),\n",
    "            nn.BatchNorm1d(int(self.hidden_dim/2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(int(self.hidden_dim/2), int(self.hidden_dim/4)),\n",
    "            nn.BatchNorm1d(int(self.hidden_dim/4)),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(int(self.hidden_dim/4), int(self.hidden_dim/8)),\n",
    "            nn.BatchNorm1d(int(self.hidden_dim/8)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Linear(int(self.hidden_dim/8), tgt_class_cnt)\n",
    "  \n",
    "#             nn.Dropout(0.5),\n",
    "           \n",
    "\n",
    "        )\n",
    "   \n",
    "    def forward(self, x):\n",
    "       \n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = self.linear_relu_stack1(x)\n",
    "        x = self.linear_relu_stack2(x)\n",
    "        x = self.linear_relu_stack2(x)\n",
    "        \n",
    "\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ab94c8e-4b00-4a52-9007-d8d1213eb57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Sequential: 1-1                        --\n",
      "|    └─Linear: 2-1                       112,603,932\n",
      "|    └─BatchNorm1d: 2-2                  21,222\n",
      "|    └─ReLU: 2-3                         --\n",
      "|    └─Linear: 2-4                       56,296,660\n",
      "|    └─BatchNorm1d: 2-5                  10,610\n",
      "|    └─ReLU: 2-6                         --\n",
      "|    └─Dropout: 2-7                      --\n",
      "├─Sequential: 1-2                        --\n",
      "|    └─Linear: 2-8                       28,148,330\n",
      "|    └─BatchNorm1d: 2-9                  10,610\n",
      "|    └─ReLU: 2-10                        --\n",
      "|    └─Linear: 2-11                      28,148,330\n",
      "|    └─BatchNorm1d: 2-12                 10,610\n",
      "|    └─ReLU: 2-13                        --\n",
      "├─Sequential: 1-3                        --\n",
      "|    └─Linear: 2-14                      14,071,512\n",
      "|    └─BatchNorm1d: 2-15                 5,304\n",
      "|    └─ReLU: 2-16                        --\n",
      "|    └─Dropout: 2-17                     --\n",
      "|    └─Linear: 2-18                      3,517,878\n",
      "|    └─BatchNorm1d: 2-19                 2,652\n",
      "|    └─ReLU: 2-20                        --\n",
      "|    └─Linear: 2-21                      879,801\n",
      "|    └─BatchNorm1d: 2-22                 1,326\n",
      "|    └─ReLU: 2-23                        --\n",
      "|    └─Dropout: 2-24                     --\n",
      "|    └─Linear: 2-25                      5,312\n",
      "=================================================================\n",
      "Total params: 243,734,089\n",
      "Trainable params: 243,734,089\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "INPUT_SIZE=131*81\n",
    "TGT_CLASS_CNT=8\n",
    "BATCH_SIZE = 16 #128\n",
    "\n",
    "mlp = Ravdess_MLP(input_size=INPUT_SIZE, tgt_class_cnt=TGT_CLASS_CNT, num_of_layers=2)\n",
    "# print(mlp)\n",
    "# params = list(mlp.parameters())\n",
    "# print(len(params))\n",
    "# print(params[0].size())  # conv1's .weight\n",
    "\n",
    "summary(mlp)\n",
    "\n",
    "for p in mlp.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "mlp = mlp.to(DEVICE)\n",
    "\n",
    "# loss_fn = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae83d023-22b0-44bf-8b8c-6ab1c3ecfb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohnik\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/mohnik/statML/runs/g2i6uahr\" target=\"_blank\">major-microwave-65</a></strong> to <a href=\"https://wandb.ai/mohnik/statML\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 8.266, Val loss: 7.632, Epoch time = 12.666s\n",
      "Epoch: 2, Train loss: 7.804, Val loss: 7.440, Epoch time = 15.007s\n",
      "Epoch: 3, Train loss: 7.466, Val loss: 7.448, Epoch time = 13.176s\n",
      "Epoch: 4, Train loss: 6.903, Val loss: 6.511, Epoch time = 15.142s\n",
      "Epoch: 5, Train loss: 6.496, Val loss: 6.415, Epoch time = 13.467s\n",
      "Epoch: 6, Train loss: 6.073, Val loss: 6.244, Epoch time = 14.561s\n",
      "Epoch: 7, Train loss: 5.758, Val loss: 6.741, Epoch time = 13.004s\n",
      "Epoch: 8, Train loss: 5.397, Val loss: 6.273, Epoch time = 14.997s\n",
      "Epoch: 9, Train loss: 5.114, Val loss: 6.237, Epoch time = 15.678s\n",
      "Epoch: 10, Train loss: 4.817, Val loss: 6.271, Epoch time = 15.910s\n",
      "Epoch: 11, Train loss: 4.669, Val loss: 5.890, Epoch time = 15.056s\n",
      "Epoch: 12, Train loss: 4.302, Val loss: 5.956, Epoch time = 13.629s\n",
      "Epoch: 13, Train loss: 4.068, Val loss: 6.075, Epoch time = 15.350s\n",
      "Epoch: 14, Train loss: 3.947, Val loss: 5.933, Epoch time = 14.171s\n",
      "Epoch: 15, Train loss: 3.687, Val loss: 5.981, Epoch time = 16.052s\n",
      "Epoch: 16, Train loss: 3.316, Val loss: 6.085, Epoch time = 13.458s\n",
      "Epoch: 17, Train loss: 3.260, Val loss: 6.011, Epoch time = 16.017s\n",
      "Epoch: 18, Train loss: 3.090, Val loss: 5.839, Epoch time = 13.112s\n",
      "Epoch: 19, Train loss: 2.891, Val loss: 5.721, Epoch time = 15.930s\n",
      "Epoch: 20, Train loss: 2.777, Val loss: 6.022, Epoch time = 15.790s\n",
      "Epoch: 21, Train loss: 2.633, Val loss: 5.670, Epoch time = 13.478s\n",
      "Epoch: 22, Train loss: 2.451, Val loss: 5.669, Epoch time = 15.746s\n",
      "Epoch: 23, Train loss: 2.344, Val loss: 5.756, Epoch time = 13.346s\n",
      "Epoch: 24, Train loss: 2.191, Val loss: 5.460, Epoch time = 15.881s\n",
      "Epoch: 25, Train loss: 2.107, Val loss: 5.641, Epoch time = 14.513s\n",
      "Epoch: 26, Train loss: 2.070, Val loss: 5.497, Epoch time = 14.711s\n",
      "Epoch: 27, Train loss: 1.831, Val loss: 5.352, Epoch time = 13.762s\n",
      "Epoch: 28, Train loss: 1.738, Val loss: 5.233, Epoch time = 14.758s\n",
      "Epoch: 29, Train loss: 1.670, Val loss: 5.143, Epoch time = 16.085s\n",
      "Epoch: 30, Train loss: 1.542, Val loss: 5.102, Epoch time = 16.288s\n",
      "Epoch: 31, Train loss: 1.492, Val loss: 5.136, Epoch time = 14.886s\n",
      "Epoch: 32, Train loss: 1.510, Val loss: 5.090, Epoch time = 14.543s\n",
      "Epoch: 33, Train loss: 1.224, Val loss: 5.137, Epoch time = 15.242s\n",
      "Epoch: 34, Train loss: 1.207, Val loss: 5.132, Epoch time = 14.055s\n",
      "Epoch: 35, Train loss: 1.204, Val loss: 5.058, Epoch time = 16.184s\n",
      "Epoch: 36, Train loss: 1.165, Val loss: 4.802, Epoch time = 13.226s\n",
      "Epoch: 37, Train loss: 1.068, Val loss: 4.579, Epoch time = 16.101s\n",
      "Epoch: 38, Train loss: 1.064, Val loss: 5.006, Epoch time = 13.440s\n",
      "Epoch: 39, Train loss: 0.963, Val loss: 4.809, Epoch time = 15.598s\n",
      "Epoch: 40, Train loss: 0.796, Val loss: 4.769, Epoch time = 16.042s\n",
      "Epoch: 41, Train loss: 0.829, Val loss: 4.844, Epoch time = 13.713s\n",
      "Epoch: 42, Train loss: 0.827, Val loss: 4.908, Epoch time = 15.934s\n",
      "Epoch: 43, Train loss: 0.801, Val loss: 4.885, Epoch time = 13.496s\n",
      "Epoch: 44, Train loss: 0.707, Val loss: 4.736, Epoch time = 15.893s\n",
      "Epoch: 45, Train loss: 0.733, Val loss: 4.483, Epoch time = 14.419s\n",
      "Epoch: 46, Train loss: 0.621, Val loss: 4.587, Epoch time = 15.397s\n",
      "Epoch: 47, Train loss: 0.581, Val loss: 4.521, Epoch time = 13.380s\n",
      "Epoch: 48, Train loss: 0.543, Val loss: 4.633, Epoch time = 15.358s\n",
      "Epoch: 49, Train loss: 0.468, Val loss: 4.671, Epoch time = 15.615s\n",
      "Epoch: 50, Train loss: 0.499, Val loss: 4.730, Epoch time = 16.596s\n",
      "Epoch: 51, Train loss: 0.483, Val loss: 4.349, Epoch time = 15.005s\n",
      "Epoch: 52, Train loss: 0.448, Val loss: 4.563, Epoch time = 13.928s\n",
      "Epoch: 53, Train loss: 0.446, Val loss: 4.486, Epoch time = 16.080s\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "import wandb\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 600\n",
    "NORM='channel'\n",
    "\n",
    "if NORM=='channel':\n",
    "    MEAN=channel_mean\n",
    "    STD=channel_std\n",
    "elif NORM=='global':\n",
    "    MEAN=global_mean\n",
    "    STD=global_std\n",
    "else:\n",
    "    MEAN=0\n",
    "    STD=1\n",
    "\n",
    "# start a new experiment\n",
    "wandb.init(project=\"statML\", entity=\"mohnik\")\n",
    "\n",
    "# capture a dictionary of hyperparameters with config\n",
    "wandb.config = {\"learning_rate\": 0.001, \"epochs\": NUM_EPOCHS, \"batch_size\": BATCH_SIZE}\n",
    "\n",
    "wandb.watch(mlp, loss_fn, log='all', log_freq=20)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(mlp, optimizer)\n",
    "    end_time = timer()\n",
    "    # if epoch%10==0:\n",
    "    #     PATH=f\"checkpoints/checkpoint_{epoch}.pt\"\n",
    "    #     torch.save({\n",
    "    #             'epoch': epoch,\n",
    "    #             'model_state_dict': transformer.state_dict(),\n",
    "    #             'optimizer_state_dict': optimizer.state_dict(),\n",
    "    #             'loss': train_loss,\n",
    "    #             }, PATH)\n",
    "    val_loss = evaluate(mlp)\n",
    "     # Where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"val_loss\": val_loss})\n",
    "    \n",
    "\n",
    "    # Optional\n",
    "    # wandb.watch(mlp)\n",
    "\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f7328c-e2ba-4079-b944-63cc54923126",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714dcef5-1926-44bb-baa4-dbb0e816694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def get_confusion_matrix(mdl, dataloader):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        mdl.eval()\n",
    "        for src, tgt in dataloader:\n",
    "            src = src.to(DEVICE)\n",
    "            tgt = tgt.to(DEVICE)\n",
    "            # print(src.shape)\n",
    "            outputs = mdl(src)\n",
    "            # labels = labels.argmax(1)\n",
    "            predicted = outputs.argmax(1)\n",
    "            y_pred.append(predicted.cpu().numpy())\n",
    "            y_true.append(tgt.cpu().numpy())\n",
    "            \n",
    "\n",
    "            total += len(src)\n",
    "            correct += (tgt == predicted).sum().item()\n",
    "    # print(np.shape(y_true))\n",
    "    y_true, y_pred = np.concatenate(y_true), np.concatenate(y_pred)\n",
    "    print(np.shape(y_true))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return cm\n",
    "\n",
    "def plot_cm(cm):\n",
    "    f, axs = plt.subplots(1,2, figsize=(12,5))\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues', ax=axs[0])\n",
    "    sns.heatmap(cm/cm.sum(1).reshape(-1,1), annot=True, cmap='Blues', ax=axs[1])\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "train_iter = RAVDESSFeatureDataset(split='train',split_by=SPLIT_BY, root_dir=root_features, mean=MEAN, std= STD)#Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "train_dataloader = DataLoader(train_iter, batch_size=1, shuffle=True)\n",
    "    \n",
    "train_cm = get_confusion_matrix(mlp,train_dataloader)\n",
    "train_acc = (train_cm*np.eye(len(train_cm))).sum()/np.sum(train_cm)\n",
    "\n",
    "print(train_cm*np.eye(len(train_cm)))\n",
    "val_iter = RAVDESSFeatureDataset(split='valid',split_by=SPLIT_BY, root_dir=root_features, mean=MEAN, std= STD)#Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "val_dataloader = DataLoader(val_iter, batch_size=1, shuffle=True)\n",
    "    \n",
    "val_cm = get_confusion_matrix(mlp,val_dataloader)\n",
    "val_acc = (val_cm*np.eye(len(val_cm))).sum()/np.sum(val_cm)\n",
    "\n",
    "\n",
    "\n",
    "plot_cm(train_cm)\n",
    "print(f'Train accuracy: {train_acc}')\n",
    "\n",
    "plot_cm(val_cm)\n",
    "print(f'Val accuracy: {val_acc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a75aaa2-9f9e-445b-a2a6-87dd279b56c1",
   "metadata": {},
   "source": [
    "## 2D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a475b167-67d2-455e-8887-5a535abe9318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Ravdess_CNN(nn.Module):\n",
    "    def __init__(self, input_size, tgt_class_cnt, num_of_layers):\n",
    "        super(Ravdess_CNN, self).__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.input_size=input_size\n",
    "        self.tgt_class_cnt=tgt_class_cnt\n",
    "        \n",
    "        self.hidden_dim=int(self.input_size/2)\n",
    "\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel \n",
    "        self.conv2_1 = nn.Conv2d(16, 32, 7,stride=1, padding=0)\n",
    "        \n",
    "        self.conv2_2 = nn.Conv2d(32, 32, 5,stride=1, padding=0)\n",
    "\n",
    "        self.conv2_3 = nn.Conv2d(32, 64, 5,stride=1, padding=0)\n",
    "\n",
    "        self.conv2_4 = nn.Conv2d(32, 64, 3,stride=1, padding=0)\n",
    "\n",
    "        self.conv2_5 = nn.Conv2d(64, 128, 3,stride=1, padding=0)\n",
    "\n",
    "        self.conv2_6 = nn.Conv2d(128, 128, 3,stride=1, padding=0)\n",
    "\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(128 * 5 * 5, 512)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(512, tgt_class_cnt)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv4(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv5(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv6(x)), (2, 2))\n",
    "\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034aee65-15c9-40a2-a1be-667de4357d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "INPUT_SIZE=131*81\n",
    "TGT_CLASS_CNT=8\n",
    "BATCH_SIZE = 16 #128\n",
    "\n",
    "cnn = Ravdess_CNN(input_size=INPUT_SIZE, tgt_class_cnt=TGT_CLASS_CNT, num_of_layers=2)\n",
    "# print(mlp)\n",
    "# params = list(mlp.parameters())\n",
    "# print(len(params))\n",
    "# print(params[0].size())  # conv1's .weight\n",
    "\n",
    "summary(cnn)\n",
    "\n",
    "for p in cnn.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "cnn = cnn.to(DEVICE)\n",
    "\n",
    "# loss_fn = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f77bbd4-824f-4176-9a70-9db3467810aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "import wandb\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 600\n",
    "NORM='channel'\n",
    "\n",
    "if NORM=='channel':\n",
    "    MEAN=channel_mean\n",
    "    STD=channel_std\n",
    "elif NORM=='global':\n",
    "    MEAN=global_mean\n",
    "    STD=global_std\n",
    "else:\n",
    "    MEAN=0\n",
    "    STD=1\n",
    "\n",
    "# start a new experiment\n",
    "wandb.init(project=\"statML\", entity=\"mohnik\")\n",
    "\n",
    "# capture a dictionary of hyperparameters with config\n",
    "wandb.config = {\"learning_rate\": 0.001, \"epochs\": NUM_EPOCHS, \"batch_size\": BATCH_SIZE}\n",
    "\n",
    "wandb.watch(cnn, loss_fn, log='all', log_freq=20)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(cnn, optimizer)\n",
    "    end_time = timer()\n",
    "    # if epoch%10==0:\n",
    "    #     PATH=f\"checkpoints/checkpoint_{epoch}.pt\"\n",
    "    #     torch.save({\n",
    "    #             'epoch': epoch,\n",
    "    #             'model_state_dict': transformer.state_dict(),\n",
    "    #             'optimizer_state_dict': optimizer.state_dict(),\n",
    "    #             'loss': train_loss,\n",
    "    #             }, PATH)\n",
    "    val_loss = evaluate(cnn)\n",
    "     # Where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"val_loss\": val_loss})\n",
    "    \n",
    "\n",
    "    # Optional\n",
    "    # wandb.watch(mlp)\n",
    "\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e392e9-c411-4164-9d54-45e757be264d",
   "metadata": {},
   "source": [
    "## 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe819ee-ef99-4ac5-a1df-6211edcd2f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Ravdess_CNN(nn.Module):\n",
    "     def __init__(self, input_size, tgt_class_cnt, num_of_layers):\n",
    "        super(Ravdess_MLP, self).__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.input_size=input_size\n",
    "        self.tgt_class_cnt=tgt_class_cnt\n",
    "        \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel nn.Conv1d(16, 33, 3, stride=2)\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
