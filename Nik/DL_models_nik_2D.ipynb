{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d313ea48-91ea-453e-aadd-e0aabe3527a9",
   "metadata": {},
   "source": [
    "## CUDA and SPLIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5b18c44-e12a-4cb6-94ef-b2e3eba6334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from statMLlib.DatasetWrapper import RAVDESSFeatureDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.empty_cache()\n",
    "SPLIT_BY = 'actor'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eaad13-b4a4-4d1b-91f5-27c13d1c4e80",
   "metadata": {},
   "source": [
    "## normalize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27faa1a1-ab9a-4c27-95eb-4fd4ce8a2ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of global mean: torch.Size([1, 1])\n",
      "Shape of global standard dev: torch.Size([1, 1])\n",
      "Shape of channel mean: torch.Size([1, 131])\n",
      "Shape of channel standard dev: torch.Size([1, 131])\n"
     ]
    }
   ],
   "source": [
    "from statMLlib import DatasetWrapper \n",
    "import imp\n",
    "imp.reload(DatasetWrapper)\n",
    "from statMLlib.DatasetWrapper import RAVDESSFeatureDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class StatsRecorder:\n",
    "    def __init__(self, red_dims=(0,1,2)):\n",
    "        \"\"\"Accumulates normalization statistics across mini-batches.\n",
    "        ref: http://notmatthancock.github.io/2017/03/23/simple-batch-stat-updates.html\n",
    "        \"\"\"\n",
    "        self.red_dims = red_dims # which mini-batch dimensions to average over\n",
    "        self.nobservations = 0   # running number of observations\n",
    "\n",
    "    def update(self, data):\n",
    "        \"\"\"\n",
    "        data: ndarray, shape (nobservations, ndimensions)\n",
    "        \"\"\"\n",
    "        # initialize stats and dimensions on first batch\n",
    "        if self.nobservations == 0:\n",
    "            self.mean = data.mean(dim=self.red_dims, keepdim=True)\n",
    "            self.std  = data.std (dim=self.red_dims,keepdim=True)\n",
    "            self.nobservations = data.shape[0]\n",
    "            self.ndimensions   = data.shape[1]\n",
    "        else:\n",
    "            if data.shape[1] != self.ndimensions:\n",
    "                raise ValueError('Data dims do not match previous observations.')\n",
    "            \n",
    "            # find mean of new mini batch\n",
    "            newmean = data.mean(dim=self.red_dims, keepdim=True)\n",
    "            newstd  = data.std(dim=self.red_dims, keepdim=True)\n",
    "            \n",
    "            # update number of observations\n",
    "            m = self.nobservations * 1.0\n",
    "            n = data.shape[0]\n",
    "\n",
    "            # update running statistics\n",
    "            tmp = self.mean\n",
    "            self.mean = m/(m+n)*tmp + n/(m+n)*newmean\n",
    "            self.std  = m/(m+n)*self.std**2 + n/(m+n)*newstd**2 +\\\n",
    "                        m*n/(m+n)**2 * (tmp - newmean)**2\n",
    "            self.std  = torch.sqrt(self.std)\n",
    "                                 \n",
    "            # update total number of seen samples\n",
    "            self.nobservations += n\n",
    "\n",
    "\n",
    "root_features='/home/spongebob*/statML_project/RAVDESS/RAVDESS-emotions-speech-audio-only-master/Audio_Speech_Actors_01-24/FeaturesAll/2d-cnn/'\n",
    "\n",
    "train_iter = RAVDESSFeatureDataset(split='train',split_by=SPLIT_BY, root_dir=root_features)#Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "train_dataloader = DataLoader(train_iter, batch_size=8, shuffle=True)\n",
    "\n",
    "# create recorders\n",
    "global_stats  = StatsRecorder()\n",
    "channel_stats = StatsRecorder(red_dims=(0,1))\n",
    "\n",
    "# step through the training dataset\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for idx,(x,y) in enumerate(iter(train_dataloader)):\n",
    "        # print(x.shape)\n",
    "        # print((torch.mean(x, axis=1)))\n",
    "        # update normalization statistics\n",
    "        # x=(x-torch.mean(x))/torch.std(x)\n",
    "        global_stats.update(x)\n",
    "        channel_stats.update(x)\n",
    "    \n",
    "# parse out both sets of stats\n",
    "global_mean,global_std = global_stats.mean,global_stats.std\n",
    "global_mean=global_mean.squeeze(0)\n",
    "global_std=global_std.squeeze(0)\n",
    "\n",
    "\n",
    "channel_mean,channel_std = channel_stats.mean,channel_stats.std\n",
    "channel_mean=channel_mean.squeeze(0)\n",
    "channel_std=channel_std.squeeze(0)\n",
    "\n",
    "print(f'Shape of global mean: {global_mean.shape}')\n",
    "print(f'Shape of global standard dev: {global_std.shape}')\n",
    "\n",
    "print(f'Shape of channel mean: {channel_mean.shape}')\n",
    "print(f'Shape of channel standard dev: {channel_std.shape}')\n",
    "# print(channel_mean)\n",
    "global_mean.to(DEVICE)\n",
    "global_std.to(DEVICE)\n",
    "\n",
    "channel_mean.to(DEVICE)\n",
    "channel_std.to(DEVICE)\n",
    "\n",
    "# print(channel_std)\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a94d4a-a8c8-4305-b81c-c86fc212ada8",
   "metadata": {},
   "source": [
    "## check sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f19fe7b-14cd-460e-9e0f-0958de3a945b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 81, 131])\n",
      "tensor(0.4497, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from statMLlib import DatasetWrapper \n",
    "import imp\n",
    "imp.reload(DatasetWrapper)\n",
    "from statMLlib.DatasetWrapper import RAVDESSFeatureDataset\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "root_features='/home/spongebob*/statML_project/RAVDESS/RAVDESS-emotions-speech-audio-only-master/Audio_Speech_Actors_01-24/FeaturesAll/2d-cnn/'\n",
    "\n",
    "train_iter = RAVDESSFeatureDataset(split='train',split_by=SPLIT_BY, root_dir=root_features, mean=channel_mean, std= channel_std)#Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "train_dataloader = DataLoader(train_iter, batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "imput_dim=(None,None)\n",
    "\n",
    "for src, tgt in train_dataloader:\n",
    "    src = src.to(DEVICE)\n",
    "    tgt = tgt.to(DEVICE)\n",
    "    \n",
    "    print(src.shape)\n",
    "    \n",
    "    imput_dim=(src.shape[1], src.shape[2])\n",
    "#     print(tgt.shape)\n",
    "#     print(torch.mean(src, axis=1).shape)\n",
    "    \n",
    "\n",
    "    print(torch.mean(src[0][0]))\n",
    "    fig=plt.figure(figsize=(8, 8))\n",
    "    # plt.plot(src.cpu().numpy()[1])\n",
    "    imgplot = plt.imshow(src.cpu().numpy()[1], cmap=plt.get_cmap( 'inferno'))\n",
    "    \n",
    "    # print(std)\n",
    "    # print(src.cpu().numpy()[5])\n",
    "\n",
    "    break\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa73e7bd-a07f-40f2-85ec-0400cef3ce8c",
   "metadata": {},
   "source": [
    "## Train Eval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61117f98-88da-42ac-bb0f-99a9b4fe9deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root_features='/home/spongebob*/statML_project/RAVDESS/RAVDESS-emotions-speech-audio-only-master/Audio_Speech_Actors_01-24/FeaturesAll/2d-cnn/'\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "\n",
    "    train_iter = RAVDESSFeatureDataset(split='train',split_by=SPLIT_BY, root_dir=root_features, mean=MEAN, std= STD)#Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "        \n",
    "#         if(NORM=='channel'):\n",
    "#             mean=channel_mean.repeat(src.shape[0],1,src.shape[2]).to(DEVICE)\n",
    "#             std=channel_std.repeat(src.shape[0],1,src.shape[2]).to(DEVICE)\n",
    "        \n",
    "#         elif NORM == 'global':\n",
    "#             mean=global_mean.repeat(src.shape[0],src.shape[1],src.shape[2]).to(DEVICE)\n",
    "#             std=global_std.repeat(src.shape[0],src.shape[1],src.shape[2]).to(DEVICE)\n",
    "            \n",
    "#         else:\n",
    "#             mean=torch.tensor(0).to(DEVICE)\n",
    "#             std=torch.tensor(1).to(DEVICE)\n",
    "            \n",
    "#         # src=(src-torch.mean(src, axis=1))/torch.std(src, axis=1)\n",
    "#         src_norm=(src-mean)/std\n",
    "        \n",
    "        logits = model(src)\n",
    "        # print(logits.shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "       \n",
    "\n",
    "        loss = loss_fn(logits, tgt)\n",
    "        # print(loss)\n",
    "        # loss = loss_fn(logits, tgt_out)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    \n",
    "    val_iter = RAVDESSFeatureDataset(split='valid',split_by=SPLIT_BY, root_dir=root_features, mean=MEAN, std= STD)#Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "        \n",
    "#         if NORM=='channel':\n",
    "#             mean=channel_mean.repeat(src.shape[0],1,src.shape[2]).to(DEVICE)\n",
    "#             std=channel_std.repeat(src.shape[0],1,src.shape[2]).to(DEVICE)\n",
    "        \n",
    "#         elif NORM == 'global':\n",
    "#             mean=global_mean.repeat(src.shape[0],src.shape[1],src.shape[2]).to(DEVICE)\n",
    "#             std=global_std.repeat(src.shape[0],src.shape[1],src.shape[2]).to(DEVICE)\n",
    "            \n",
    "#         else:\n",
    "#             mean=torch.tensor(0).to(DEVICE)\n",
    "#             std=torch.tensor(1).to(DEVICE)\n",
    "            \n",
    "#         # src=(src-torch.mean(src))/torch.std(src)\n",
    "#         src_norm=(src-mean)/std\n",
    "\n",
    "        logits = model(src)\n",
    "  \n",
    "        loss = loss_fn(logits, tgt)\n",
    "     \n",
    "        losses += loss.item()\n",
    "    \n",
    "    \n",
    "    \n",
    "    return losses / len(val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920ada28-39e6-445b-9158-8ce68bfa78d2",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7704dce6-eb1b-4ea1-86b1-8ce20bc312fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Ravdess_MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, tgt_class_cnt, num_of_layers):\n",
    "        super(Ravdess_MLP, self).__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.input_size=input_size\n",
    "        self.tgt_class_cnt=tgt_class_cnt\n",
    "        \n",
    "        self.hidden_dim=input_size*4#int(self.input_size/2)\n",
    "                               \n",
    "        self.linear_relu_stack1 = nn.Sequential(\n",
    "\n",
    "           \n",
    "            nn.Linear(input_size, int(self.hidden_dim/2)),\n",
    "            nn.BatchNorm1d(int(self.hidden_dim/2)),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(int(self.hidden_dim/2), self.hidden_dim),\n",
    "            nn.BatchNorm1d(self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            \n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.BatchNorm1d(self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.1),\n",
    "        )\n",
    "        \n",
    "#         self.hidden = []\n",
    "#         for k in range(num_of_layers):\n",
    "#             self.hidden.append(nn.Linear(self.hidden_dim, self.hidden_dim))\n",
    "#             self.hidden.append(nn.BatchNorm1d(self.hidden_dim))\n",
    "#             self.hidden.append(nn.ReLU())\n",
    "#             # self.hidden.append(nn.Dropout(0.1))\n",
    "\n",
    "\n",
    "#         self.linear_relu_stack2 = nn.Sequential(*self.hidden)\n",
    "        \n",
    "        self.linear_relu_stack3 = nn.Sequential(\n",
    "  \n",
    "            # nn.Linear(self.hidden_dim, int(self.hidden_dim)),\n",
    "            # nn.BatchNorm1d(int(self.hidden_dim)),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(int(self.hidden_dim), int(self.hidden_dim/2)),\n",
    "            nn.BatchNorm1d(int(self.hidden_dim/2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "#             nn.Linear(int(self.hidden_dim/4), int(self.hidden_dim/8)),\n",
    "#             nn.BatchNorm1d(int(self.hidden_dim/8)),\n",
    "#             nn.ReLU(),\n",
    "            # nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Linear(int(self.hidden_dim/2), tgt_class_cnt)\n",
    "  \n",
    "#             nn.Dropout(0.5),\n",
    "           \n",
    "\n",
    "        )\n",
    "   \n",
    "    def forward(self, x):\n",
    "       \n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = self.linear_relu_stack1(x)\n",
    "        # x = self.linear_relu_stack2(x)\n",
    "        x = self.linear_relu_stack3(x)\n",
    "        \n",
    "\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab94c8e-4b00-4a52-9007-d8d1213eb57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "INPUT_SIZE=imput_dim[0]*imput_dim[1]\n",
    "TGT_CLASS_CNT=8\n",
    "\n",
    "mlp = Ravdess_MLP(input_size=INPUT_SIZE, tgt_class_cnt=TGT_CLASS_CNT, num_of_layers=2)\n",
    "# print(mlp)\n",
    "# params = list(mlp.parameters())\n",
    "# print(len(params))\n",
    "# print(params[0].size())  # conv1's .weight\n",
    "\n",
    "summary(mlp)\n",
    "print(mlp)\n",
    "for p in mlp.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "mlp = mlp.to(DEVICE)\n",
    "\n",
    "# loss_fn = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae83d023-22b0-44bf-8b8c-6ab1c3ecfb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "import wandb\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 18\n",
    "NORM='global'#'channel'\n",
    "BATCH_SIZE = 8 #128\n",
    "\n",
    "if NORM=='channel':\n",
    "    MEAN=channel_mean\n",
    "    STD=channel_std\n",
    "elif NORM=='global':\n",
    "    MEAN=global_mean\n",
    "    STD=global_std\n",
    "else:\n",
    "    MEAN=0\n",
    "    STD=1\n",
    "\n",
    "# start a new experiment\n",
    "wandb.init(project=\"statML\", entity=\"mohnik\")\n",
    "\n",
    "#â€ƒcapture a dictionary of hyperparameters with config\n",
    "wandb.config = {\"learning_rate\": 0.0001, \"epochs\": NUM_EPOCHS, \"batch_size\": BATCH_SIZE}\n",
    "\n",
    "wandb.watch(mlp, loss_fn, log='all', log_freq=20)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(mlp, optimizer)\n",
    "    end_time = timer()\n",
    "    # if epoch%10==0:\n",
    "    #     PATH=f\"checkpoints/checkpoint_{epoch}.pt\"\n",
    "    #     torch.save({\n",
    "    #             'epoch': epoch,\n",
    "    #             'model_state_dict': transformer.state_dict(),\n",
    "    #             'optimizer_state_dict': optimizer.state_dict(),\n",
    "    #             'loss': train_loss,\n",
    "    #             }, PATH)\n",
    "    val_loss = evaluate(mlp)\n",
    "     # Where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"val_loss\": val_loss})\n",
    "    \n",
    "\n",
    "    # Optional\n",
    "    # wandb.watch(mlp)\n",
    "\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f7328c-e2ba-4079-b944-63cc54923126",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714dcef5-1926-44bb-baa4-dbb0e816694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def get_confusion_matrix(mdl, dataloader):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        mdl.eval()\n",
    "        for src, tgt in dataloader:\n",
    "            src = src.to(DEVICE)\n",
    "            tgt = tgt.to(DEVICE)\n",
    "            # print(src.shape)\n",
    "            outputs = mdl(src)\n",
    "            # labels = labels.argmax(1)\n",
    "            predicted = outputs.argmax(1)\n",
    "            y_pred.append(predicted.cpu().numpy())\n",
    "            y_true.append(tgt.cpu().numpy())\n",
    "            \n",
    "\n",
    "            total += len(src)\n",
    "            correct += (tgt == predicted).sum().item()\n",
    "    # print(np.shape(y_true))\n",
    "    y_true, y_pred = np.concatenate(y_true), np.concatenate(y_pred)\n",
    "    print(np.shape(y_true))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return cm\n",
    "\n",
    "def plot_cm(cm):\n",
    "    f, axs = plt.subplots(1,2, figsize=(12,5))\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues', ax=axs[0])\n",
    "    sns.heatmap(cm/cm.sum(1).reshape(-1,1), annot=True, cmap='Blues', ax=axs[1])\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "train_iter = RAVDESSFeatureDataset(split='train',split_by=SPLIT_BY, root_dir=root_features, mean=MEAN, std= STD)#Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "train_dataloader = DataLoader(train_iter, batch_size=1, shuffle=True)\n",
    "    \n",
    "train_cm = get_confusion_matrix(mlp,train_dataloader)\n",
    "train_acc = (train_cm*np.eye(len(train_cm))).sum()/np.sum(train_cm)\n",
    "\n",
    "print(train_cm*np.eye(len(train_cm)))\n",
    "val_iter = RAVDESSFeatureDataset(split='valid',split_by=SPLIT_BY, root_dir=root_features, mean=MEAN, std= STD)#Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "val_dataloader = DataLoader(val_iter, batch_size=1, shuffle=True)\n",
    "    \n",
    "val_cm = get_confusion_matrix(mlp,val_dataloader)\n",
    "val_acc = (val_cm*np.eye(len(val_cm))).sum()/np.sum(val_cm)\n",
    "\n",
    "\n",
    "\n",
    "plot_cm(train_cm)\n",
    "print(f'Train accuracy: {train_acc}')\n",
    "\n",
    "plot_cm(val_cm)\n",
    "print(f'Val accuracy: {val_acc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0850a8a-0c88-495a-a8f8-8cd6334f6ff8",
   "metadata": {},
   "source": [
    "## 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394f2188-d10b-41c3-9deb-be0194407b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Ravdess_CNN_1D(nn.Module):\n",
    "    def __init__(self, input_size, tgt_class_cnt, num_of_layers=None):\n",
    "        super(Ravdess_CNN_1D, self).__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.input_size=input_size\n",
    "        self.tgt_class_cnt=tgt_class_cnt\n",
    "        \n",
    "        # self.hidden_dim=int(self.input_size/2)\n",
    "\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel \n",
    "        self.conv1 = nn.Conv1d(1, 32, 5,stride=2, padding=0)\n",
    "        \n",
    "        # self.conv2 = nn.Conv1d(32, 32, 5,stride=1, padding=0)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(32, 32, 5,stride=1, padding=0)\n",
    "\n",
    "        self.conv4 = nn.Conv1d(32, 64, 3,stride=1, padding=0)\n",
    "\n",
    "        # self.conv5 = nn.Conv1d(64, 128, 3,stride=1, padding=0)\n",
    "\n",
    "        self.conv6 = nn.Conv1d(64, 64, 2, stride=1, padding=0)\n",
    "\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(64, 128)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(128, tgt_class_cnt)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool1d(F.relu(self.conv1(x)), 1)\n",
    "        # x = F.max_pool1d(F.relu(self.conv2(x)), 2)\n",
    "        x = F.max_pool1d(F.relu(self.conv3(x)), 1)\n",
    "        x = F.max_pool1d(F.relu(self.conv4(x)), 2)\n",
    "        # x = F.max_pool1d(F.relu(self.conv5(x)), 2)\n",
    "        x = F.adaptive_max_pool1d(F.relu(self.conv6(x)), 1)\n",
    "        # print(x.shape)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5574e42b-5f3f-47a8-b3eb-e1c7c6922f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "INPUT_SIZE=INPUT_SIZE=imput_dim[0]*imput_dim[1]\n",
    "TGT_CLASS_CNT=8\n",
    "BATCH_SIZE = 64 #128\n",
    "\n",
    "cnn_1d = Ravdess_CNN_1D(input_size=INPUT_SIZE, tgt_class_cnt=TGT_CLASS_CNT)\n",
    "# print(mlp)\n",
    "# params = list(mlp.parameters())\n",
    "# print(len(params))\n",
    "# print(params[0].size())  # conv1's .weight\n",
    "\n",
    "summary(cnn_1d)\n",
    "\n",
    "for p in cnn_1d.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "cnn_1d = cnn_1d.to(DEVICE)\n",
    "print(cnn_1d)\n",
    "# loss_fn = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(cnn_1d.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1991052-fc89-46fe-a842-d4f9c4603af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "import wandb\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "NORM=None\n",
    "\n",
    "if NORM=='channel':\n",
    "    MEAN=channel_mean\n",
    "    STD=channel_std\n",
    "elif NORM=='global':\n",
    "    MEAN=global_mean\n",
    "    STD=global_std\n",
    "else:\n",
    "    MEAN=0\n",
    "    STD=1\n",
    "\n",
    "# start a new experiment\n",
    "wandb.init(project=\"statML\", entity=\"mohnik\")\n",
    "\n",
    "#â€ƒcapture a dictionary of hyperparameters with config\n",
    "wandb.config = {\"learning_rate\": 0.0001, \"epochs\": NUM_EPOCHS, \"batch_size\": BATCH_SIZE}\n",
    "\n",
    "wandb.watch(cnn_1d, loss_fn, log='all', log_freq=20)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(cnn_1d, optimizer)\n",
    "    end_time = timer()\n",
    "    # if epoch%10==0:\n",
    "    #     PATH=f\"checkpoints/checkpoint_{epoch}.pt\"\n",
    "    #     torch.save({\n",
    "    #             'epoch': epoch,\n",
    "    #             'model_state_dict': transformer.state_dict(),\n",
    "    #             'optimizer_state_dict': optimizer.state_dict(),\n",
    "    #             'loss': train_loss,\n",
    "    #             }, PATH)\n",
    "    val_loss = evaluate(cnn_1d)\n",
    "     # Where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"train_loss\": [train_loss, val_loss], \"val_loss\": val_loss})\n",
    "    \n",
    "\n",
    "    # Optional\n",
    "    # wandb.watch(mlp)\n",
    "\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05d75f7-175f-4a59-993b-cb4674251faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def get_confusion_matrix(mdl, dataloader):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        mdl.eval()\n",
    "        for src, tgt in dataloader:\n",
    "            src = src.to(DEVICE)\n",
    "            tgt = tgt.to(DEVICE)\n",
    "            # print(src.shape)\n",
    "            outputs = mdl(src)\n",
    "            # labels = labels.argmax(1)\n",
    "            predicted = outputs.argmax(1)\n",
    "            y_pred.append(predicted.cpu().numpy())\n",
    "            y_true.append(tgt.cpu().numpy())\n",
    "            \n",
    "\n",
    "            total += len(src)\n",
    "            correct += (tgt == predicted).sum().item()\n",
    "    print(np.shape(y_true))\n",
    "    y_true, y_pred = np.concatenate(y_true), np.concatenate(y_pred)\n",
    "    print(np.shape(y_true))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return cm\n",
    "\n",
    "def plot_cm(cm):\n",
    "    f, axs = plt.subplots(1,2, figsize=(12,5))\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues', ax=axs[0])\n",
    "    sns.heatmap(cm/cm.sum(1).reshape(-1,1), annot=True, cmap='Blues', ax=axs[1])\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "train_iter = RAVDESSFeatureDataset(split='train',split_by=SPLIT_BY, root_dir=root_features, mean=MEAN, std= STD)#Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "train_dataloader = DataLoader(train_iter, batch_size=1, shuffle=True)\n",
    "    \n",
    "train_cm = get_confusion_matrix(cnn_1d,train_dataloader)\n",
    "train_acc = (train_cm*np.eye(len(train_cm))).sum()/np.sum(train_cm)\n",
    "\n",
    "print(train_cm*np.eye(len(train_cm)))\n",
    "val_iter = RAVDESSFeatureDataset(split='valid',split_by=SPLIT_BY, root_dir=root_features, mean=MEAN, std= STD)#Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "val_dataloader = DataLoader(val_iter, batch_size=1, shuffle=True)\n",
    "    \n",
    "val_cm = get_confusion_matrix(cnn_1d,val_dataloader)\n",
    "val_acc = (val_cm*np.eye(len(val_cm))).sum()/np.sum(val_cm)\n",
    "\n",
    "\n",
    "\n",
    "plot_cm(train_cm)\n",
    "print(f'Train accuracy: {train_acc}')\n",
    "\n",
    "plot_cm(val_cm)\n",
    "print(f'Val accuracy: {val_acc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a75aaa2-9f9e-445b-a2a6-87dd279b56c1",
   "metadata": {},
   "source": [
    "## 2D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a475b167-67d2-455e-8887-5a535abe9318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Ravdess_CNN(nn.Module):\n",
    "    def __init__(self, input_size, tgt_class_cnt, num_of_layers):\n",
    "        super(Ravdess_CNN, self).__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.input_size=input_size\n",
    "        self.tgt_class_cnt=tgt_class_cnt\n",
    "        \n",
    "        self.hidden_dim=int(self.input_size/2)\n",
    "\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel \n",
    "        self.conv1 = nn.Conv2d(1, 64, 5,stride=(2,2), padding=0)\n",
    "        \n",
    "        # self.conv2 = nn.Conv2d(32, 32, 5,stride=(2,2), padding=0)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 64, 5,stride=(1,1), padding=0)\n",
    "\n",
    "        # self.conv4 = nn.Conv2d(64, 64, 3,stride=(1,1), padding=0)\n",
    "\n",
    "        # self.conv5 = nn.Conv2d(64, 128, 3,stride=(1,1), padding=0)\n",
    "\n",
    "        self.conv6 = nn.Conv2d(64, 64, 3,stride=(1,1), padding=0)\n",
    "\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(64, 32)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(32, tgt_class_cnt)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = x.unsqueeze(1)\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (1, 1))\n",
    "        # x = F.max_pool2d(F.relu(self.conv2(x)), (1, 1))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "        # x = F.max_pool2d(F.relu(self.conv4(x)), (1, 1))\n",
    "        # x = F.max_pool2d(F.relu(self.conv5(x)), (2, 2))\n",
    "        x = F.adaptive_max_pool2d(F.relu(self.conv6(x)), 1)\n",
    "        x = F.dropout(x, 0.3)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "034aee65-15c9-40a2-a1be-667de4357d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "â”œâ”€Conv2d: 1-1                            1,664\n",
      "â”œâ”€Conv2d: 1-2                            102,464\n",
      "â”œâ”€Conv2d: 1-3                            36,928\n",
      "â”œâ”€Linear: 1-4                            2,080\n",
      "â”œâ”€Linear: 1-5                            264\n",
      "=================================================================\n",
      "Total params: 143,400\n",
      "Trainable params: 143,400\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "INPUT_SIZE=INPUT_SIZE=imput_dim[0]*imput_dim[1]\n",
    "TGT_CLASS_CNT=8\n",
    "BATCH_SIZE = 16 #128\n",
    "\n",
    "cnn = Ravdess_CNN(input_size=INPUT_SIZE, tgt_class_cnt=TGT_CLASS_CNT, num_of_layers=2)\n",
    "# print(mlp)\n",
    "# params = list(mlp.parameters())\n",
    "# print(len(params))\n",
    "# print(params[0].size())  # conv1's .weight\n",
    "\n",
    "summary(cnn)\n",
    "\n",
    "for p in cnn.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "cnn = cnn.to(DEVICE)\n",
    "\n",
    "# loss_fn = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3f77bbd4-824f-4176-9a70-9db3467810aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:87arov0p) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 623053... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_loss</td><td>â–ˆâ–ˆâ–‡â–‡â–†â–…â–…â–„â–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–</td></tr><tr><td>val_loss</td><td>â–‚â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–…â–„â–†â–…â–„â–„â–„â–…â–…â–…â–…â–†â–…â–†â–‡â–†â–†â–†â–†â–‡â–ˆâ–ˆâ–ˆâ–‡</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>train_loss</td><td>0.05797</td></tr><tr><td>val_loss</td><td>4.97705</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">flowing-mountain-149</strong>: <a href=\"https://wandb.ai/mohnik/statML/runs/87arov0p\" target=\"_blank\">https://wandb.ai/mohnik/statML/runs/87arov0p</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211127_124640-87arov0p/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:87arov0p). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/mohnik/statML/runs/3dfmeux3\" target=\"_blank\">grateful-sunset-150</a></strong> to <a href=\"https://wandb.ai/mohnik/statML\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 2.075, Val loss: 2.077, Epoch time = 0.522s\n",
      "Epoch: 2, Train loss: 2.044, Val loss: 2.037, Epoch time = 0.506s\n",
      "Epoch: 3, Train loss: 2.021, Val loss: 2.052, Epoch time = 0.496s\n",
      "Epoch: 4, Train loss: 2.003, Val loss: 2.017, Epoch time = 0.504s\n",
      "Epoch: 5, Train loss: 1.965, Val loss: 1.944, Epoch time = 0.499s\n",
      "Epoch: 6, Train loss: 1.918, Val loss: 1.917, Epoch time = 0.491s\n",
      "Epoch: 7, Train loss: 1.840, Val loss: 1.866, Epoch time = 0.501s\n",
      "Epoch: 8, Train loss: 1.810, Val loss: 1.916, Epoch time = 0.505s\n",
      "Epoch: 9, Train loss: 1.741, Val loss: 1.797, Epoch time = 0.511s\n",
      "Epoch: 10, Train loss: 1.656, Val loss: 1.767, Epoch time = 0.509s\n",
      "Epoch: 11, Train loss: 1.590, Val loss: 1.731, Epoch time = 0.504s\n",
      "Epoch: 12, Train loss: 1.492, Val loss: 1.730, Epoch time = 0.502s\n",
      "Epoch: 13, Train loss: 1.450, Val loss: 1.829, Epoch time = 0.499s\n",
      "Epoch: 14, Train loss: 1.412, Val loss: 1.709, Epoch time = 0.499s\n",
      "Epoch: 15, Train loss: 1.310, Val loss: 1.684, Epoch time = 0.507s\n",
      "Epoch: 16, Train loss: 1.247, Val loss: 1.693, Epoch time = 0.499s\n",
      "Epoch: 17, Train loss: 1.131, Val loss: 1.995, Epoch time = 0.520s\n",
      "Epoch: 18, Train loss: 1.096, Val loss: 1.810, Epoch time = 0.492s\n",
      "Epoch: 19, Train loss: 1.067, Val loss: 1.945, Epoch time = 0.494s\n",
      "Epoch: 20, Train loss: 1.036, Val loss: 1.935, Epoch time = 0.490s\n",
      "Epoch: 21, Train loss: 0.926, Val loss: 1.862, Epoch time = 0.486s\n",
      "Epoch: 22, Train loss: 0.907, Val loss: 2.139, Epoch time = 0.496s\n",
      "Epoch: 23, Train loss: 0.799, Val loss: 1.921, Epoch time = 0.491s\n",
      "Epoch: 24, Train loss: 0.765, Val loss: 1.882, Epoch time = 0.501s\n",
      "Epoch: 25, Train loss: 0.708, Val loss: 2.126, Epoch time = 0.491s\n",
      "Epoch: 26, Train loss: 0.685, Val loss: 1.954, Epoch time = 0.501s\n",
      "Epoch: 27, Train loss: 0.646, Val loss: 2.209, Epoch time = 0.497s\n",
      "Epoch: 28, Train loss: 0.592, Val loss: 2.092, Epoch time = 0.491s\n",
      "Epoch: 29, Train loss: 0.560, Val loss: 2.141, Epoch time = 0.493s\n",
      "Epoch: 30, Train loss: 0.535, Val loss: 2.416, Epoch time = 0.489s\n",
      "Epoch: 31, Train loss: 0.524, Val loss: 2.326, Epoch time = 0.495s\n",
      "Epoch: 32, Train loss: 0.543, Val loss: 2.375, Epoch time = 0.508s\n",
      "Epoch: 33, Train loss: 0.481, Val loss: 2.011, Epoch time = 0.494s\n",
      "Epoch: 34, Train loss: 0.466, Val loss: 2.630, Epoch time = 0.500s\n",
      "Epoch: 35, Train loss: 0.470, Val loss: 2.347, Epoch time = 0.505s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_620837/2536415591.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# if epoch%10==0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_620837/1417876673.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/irl_nik/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/irl_nik/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/irl_nik/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/irl_nik/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/statML_project/statMLlib/DatasetWrapper.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mwave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mpickle_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mloaded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "import wandb\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 600\n",
    "NORM='channel'\n",
    "\n",
    "if NORM=='channel':\n",
    "    MEAN=channel_mean\n",
    "    STD=channel_std\n",
    "elif NORM=='global':\n",
    "    MEAN=global_mean\n",
    "    STD=global_std\n",
    "else:\n",
    "    MEAN=0\n",
    "    STD=1\n",
    "\n",
    "# start a new experiment\n",
    "wandb.init(project=\"statML\", entity=\"mohnik\")\n",
    "\n",
    "#â€ƒcapture a dictionary of hyperparameters with config\n",
    "wandb.config = {\"learning_rate\": 0.001, \"epochs\": NUM_EPOCHS, \"batch_size\": BATCH_SIZE}\n",
    "\n",
    "wandb.watch(cnn, loss_fn, log='all', log_freq=20)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(cnn, optimizer)\n",
    "    end_time = timer()\n",
    "    # if epoch%10==0:\n",
    "    #     PATH=f\"checkpoints/checkpoint_{epoch}.pt\"\n",
    "    #     torch.save({\n",
    "    #             'epoch': epoch,\n",
    "    #             'model_state_dict': transformer.state_dict(),\n",
    "    #             'optimizer_state_dict': optimizer.state_dict(),\n",
    "    #             'loss': train_loss,\n",
    "    #             }, PATH)\n",
    "    val_loss = evaluate(cnn)\n",
    "     # Where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"val_loss\": val_loss})\n",
    "    \n",
    "\n",
    "    # Optional\n",
    "    # wandb.watch(mlp)\n",
    "\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2570209a-c72f-4d93-b30d-098f8f5f60bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
